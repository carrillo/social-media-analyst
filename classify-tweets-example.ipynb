{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal\n",
    "\n",
    "We have collected a number of tweets and dumped them in a database. If not, check out the \"gather-twitter-info-example\" notebook. We want to get an idea what people in the network are talking about, but we don't want to go through the tweets manually. Let's use the magic of machine learning instead: We learn a very simple text classifier on example texts and classify the tweets into these categories. \n",
    "\n",
    "### Train classifier\n",
    "\n",
    "Let's train the text classifier on ~18000 newsgroup posts on 20 different topics (http://scikit-learn.org/stable/datasets/twenty_newsgroups.html). This data can be easily loaded from the scikit-learn library.  \n",
    "\n",
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    " \n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=32)\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True, random_state=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train classifier\n",
    "\n",
    "In the best case scenario this text classifier should be able to classify tweets from the pre-filtered twitter stream directly. So speed does matter. Therefore let's use a simple linear classifier: A multinomial naive bayesian classifier (http://scikit-learn.org/stable/modules/naive_bayes.html). To be able to classify text we first want to create a sparse vector representation of the documents (http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) and tf-idf transform (https://en.wikipedia.org/wiki/Tf%E2%80%93idf) to take relative term frequency (tf) and inverse document frequency (idf) into account (http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html). \n",
    "\n",
    "We combine the CountVectorizer, TfidfTransformer and Classifier into a scikit-learn pipeline and identify the optimal set of meta parameters using 5-fold cross validation combined with a grid search. The parameters we are optimizing for are i) n-gram length for feature extraction (1 word or 1 and 2 words) and ii) the additive smoothing term for the naive bayes classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:  6.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score 0.917093866007 of estimator Pipeline(steps=[('vect', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        st...alse,\n",
      "         use_idf=True)), ('nb', MultinomialNB(alpha=0.001, class_prior=None, fit_prior=True))])\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.message_classifier import MessageClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "tc = MessageClassifier()\n",
    "pipeline=Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('nb', MultinomialNB())])\n",
    "param_grid={'vect__ngram_range': [(1, 1),(1, 2)], 'nb__alpha': [10**-5,10**-4,10**-3,10**-2,10**-1]}\n",
    "tc.train(train_X=twenty_train.data, train_y=twenty_train.target, labels=twenty_train.target_names, pipeline=pipeline, param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation reports an accuracy of 0.92 over all classes. Let's measure the performance on the hold out set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tc.test(valid_X=twenty_test.data, valid_y=twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
