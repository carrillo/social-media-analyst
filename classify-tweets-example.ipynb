{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal\n",
    "\n",
    "We have collected a number of tweets and dumped them in a database. If not, check out the \"gather-twitter-info-example\" notebook. We want to get an idea what people in the network are talking about, but we don't want to go through the tweets manually. Let's use the magic of machine learning instead: We learn a very simple text classifier on example texts and classify the tweets into these categories. \n",
    "\n",
    "### Train classifier\n",
    "\n",
    "Let's train the text classifier on ~18000 newsgroup posts on 20 different topics (http://scikit-learn.org/stable/datasets/twenty_newsgroups.html). This data can be easily loaded from the scikit-learn library.  \n",
    "\n",
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    " \n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=32)\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True, random_state=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train classifier\n",
    "\n",
    "In the best case scenario this text classifier should be able to classify tweets from the pre-filtered twitter stream directly. So speed does matter. Therefore let's use a simple linear classifier: A multinomial naive bayesian classifier (http://scikit-learn.org/stable/modules/naive_bayes.html). To be able to classify text we first want to create a sparse vector representation of the documents (http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) and tf-idf transform (https://en.wikipedia.org/wiki/Tf%E2%80%93idf) to take relative term frequency (tf) and inverse document frequency (idf) into account (http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html). \n",
    "\n",
    "We combine the CountVectorizer, TfidfTransformer and Classifier into a scikit-learn pipeline and identify the optimal set of meta parameters using 5-fold cross validation combined with a grid search. The parameters we are optimizing for are i) n-gram length for feature extraction (1 word or 1 and 2 words) and ii) the additive smoothing term for the naive bayes classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:  8.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score 0.917093866007 of estimator Pipeline(steps=[('vect', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        st...alse,\n",
      "         use_idf=True)), ('nb', MultinomialNB(alpha=0.001, class_prior=None, fit_prior=True))])\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.82      0.82      0.82       319\n",
      "           comp.graphics       0.69      0.74      0.71       389\n",
      " comp.os.ms-windows.misc       0.73      0.62      0.67       394\n",
      "comp.sys.ibm.pc.hardware       0.64      0.76      0.69       392\n",
      "   comp.sys.mac.hardware       0.82      0.82      0.82       385\n",
      "          comp.windows.x       0.83      0.79      0.81       395\n",
      "            misc.forsale       0.86      0.83      0.84       390\n",
      "               rec.autos       0.88      0.89      0.89       396\n",
      "         rec.motorcycles       0.95      0.94      0.95       398\n",
      "      rec.sport.baseball       0.96      0.92      0.94       397\n",
      "        rec.sport.hockey       0.95      0.97      0.96       399\n",
      "               sci.crypt       0.86      0.92      0.89       396\n",
      "         sci.electronics       0.79      0.74      0.77       393\n",
      "                 sci.med       0.88      0.83      0.85       396\n",
      "               sci.space       0.89      0.89      0.89       394\n",
      "  soc.religion.christian       0.85      0.95      0.90       398\n",
      "      talk.politics.guns       0.81      0.91      0.86       364\n",
      "   talk.politics.mideast       0.97      0.93      0.95       376\n",
      "      talk.politics.misc       0.78      0.70      0.74       310\n",
      "      talk.religion.misc       0.75      0.65      0.70       251\n",
      "\n",
      "             avg / total       0.84      0.84      0.84      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.message_classifier import MessageClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "tc = MessageClassifier()\n",
    "pipeline=Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('nb', MultinomialNB())])\n",
    "param_grid={'vect__ngram_range': [(1, 1),(1, 2)], 'nb__alpha': [10**-5,10**-4,10**-3,10**-2,10**-1]}\n",
    "tc.train(train_X=twenty_train.data, train_y=twenty_train.target, labels=twenty_train.target_names, pipeline=pipeline, param_grid=param_grid)\n",
    "\n",
    "tc.test(valid_X=twenty_test.data, valid_y=twenty_test.target) # Test performace on hold-out set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation reports an average f1-score (weighted average of precission and recall) of 0.XX. This is classification performance is slighty too optimistic: The prediction on the hold out set reports an f1-score of 0.84."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the classifier\n",
    "To be able to use the classifier in different applications. Let's save it. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
